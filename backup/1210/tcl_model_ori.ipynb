{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20516054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please compile source files before using functions CUDA extension.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/usr/lib/python36.zip',\n",
       " '/usr/lib/python3.6',\n",
       " '/usr/lib/python3.6/lib-dynload',\n",
       " '',\n",
       " '/usr/local/lib/python3.6/dist-packages',\n",
       " '/usr/local/lib/python3.6/dist-packages/pointnet2-0.0.0-py3.6-linux-x86_64.egg',\n",
       " '/usr/local/lib/python3.6/dist-packages/knn_pytorch-0.1-py3.6-linux-x86_64.egg',\n",
       " '/usr/lib/python3/dist-packages',\n",
       " '/usr/local/lib/python3.6/dist-packages/IPython/extensions',\n",
       " '/home/po/.ipython',\n",
       " '/home/po/TM5/s4g-release/inference/grasp_proposal/network_models',\n",
       " '/home/po/TM5/graspnetAPI/graspnetAPI',\n",
       " '/home/po/TM5']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os, sys\n",
    "import sys\n",
    "from torchsummary import summary\n",
    "# sys.path.append('/home/po/TM5/Pointnet_Pointnet2_pytorch')\n",
    "sys.path.append('/home/po/TM5/s4g-release/inference/grasp_proposal/network_models')\n",
    "sys.path.append('/home/po/TM5/graspnetAPI/graspnetAPI')\n",
    "sys.path.append('/home/po/TM5')\n",
    "from nn_utils.mlp import SharedMLP\n",
    "from pointnet2_utils.modules import PointNetSAModule, PointnetFPModule, PointNetSAAvgModule\n",
    "from nn_utils.functional import smooth_cross_entropy\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0f56d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 2021-11-29 22:53:29,218 - rigid_transformations - Failed to import geometry msgs in rigid_transformations.py.\n",
      "WARNING - 2021-11-29 22:53:29,219 - rigid_transformations - Failed to import ros dependencies in rigid_transforms.py\n",
      "WARNING - 2021-11-29 22:53:29,219 - rigid_transformations - autolab_core not installed as catkin package, RigidTransform ros methods will be unavailable\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('/home/po/TM5/graspnet-baseline')\n",
    "import scipy.io as scio\n",
    "from dataset.graspnet_dataset1 import GraspNetDataset, collate_fn, load_grasp_labels\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a4217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNet2(nn.Module):\n",
    "    \"\"\"PointNet++ part segmentation with single-scale grouping\n",
    "\n",
    "    PointNetSA: PointNet Set Abstraction Layer\n",
    "    PointNetFP: PointNet Feature Propagation Layer\n",
    "\n",
    "    Args:\n",
    "        score_classes (int): the number of grasp score classes\n",
    "        num_centroids (tuple of int): the numbers of centroids to sample in each set abstraction module\n",
    "        radius (tuple of float): a tuple of radius to query neighbours in each set abstraction module\n",
    "        num_neighbours (tuple of int): the numbers of neighbours to query for each centroid\n",
    "        sa_channels (tuple of tuple of int): the numbers of channels within each set abstraction module\n",
    "        fp_channels (tuple of tuple of int): the numbers of channels for feature propagation (FP) module\n",
    "        num_fp_neighbours (tuple of int): the numbers of nearest neighbor used in FP\n",
    "        seg_channels (tuple of int): the numbers of channels in segmentation mlp\n",
    "        dropout_prob (float): the probability to dropout input features\n",
    "\n",
    "    References:\n",
    "        https://github.com/charlesq34/pointnet2/blob/master/models/pointnet2_part_seg.py\n",
    "\n",
    "    \"\"\"\n",
    "    _SA_MODULE = PointNetSAModule\n",
    "    _FP_MODULE = PointnetFPModule\n",
    "\n",
    "    def __init__(self,\n",
    "                 score_classes,\n",
    "                 num_centroids=(10240, 1024, 128, 0),\n",
    "                 radius=(0.2, 0.3, 0.4, -1.0),\n",
    "                 num_neighbours=(64, 64, 64, -1),\n",
    "                 sa_channels=((32, 32, 64), (64, 64, 128), (128, 128, 256), (256, 512, 1024)),\n",
    "                 fp_channels=((256, 256), (256, 128), (128, 128), (64, 64, 64)),\n",
    "                 num_fp_neighbours=(0, 3, 3, 3),\n",
    "                 seg_channels=(128,),\n",
    "                 num_removal_directions=5,\n",
    "                 dropout_prob=0.5):\n",
    "        super(PointNet2, self).__init__()\n",
    "\n",
    "        # Sanity check\n",
    "        num_sa_layers = len(num_centroids)\n",
    "        num_fp_layers = len(fp_channels)\n",
    "        assert len(radius) == num_sa_layers\n",
    "        assert len(num_neighbours) == num_sa_layers\n",
    "        assert len(sa_channels) == num_sa_layers\n",
    "        assert num_sa_layers == num_fp_layers\n",
    "        assert len(num_fp_neighbours) == num_fp_layers\n",
    "\n",
    "        # Set Abstraction Layers\n",
    "        feature_channels = 0\n",
    "        self.sa_modules = nn.ModuleList()\n",
    "        for ind in range(num_sa_layers):\n",
    "            sa_module = self._SA_MODULE(in_channels=feature_channels,\n",
    "                                        mlp_channels=sa_channels[ind],\n",
    "                                        num_centroids=num_centroids[ind],\n",
    "                                        radius=radius[ind],\n",
    "                                        num_neighbours=num_neighbours[ind],\n",
    "                                        use_xyz=True)\n",
    "            self.sa_modules.append(sa_module)\n",
    "            feature_channels = sa_channels[ind][-1]\n",
    "\n",
    "        inter_channels = [0]\n",
    "        inter_channels.extend([x[-1] for x in sa_channels])\n",
    "\n",
    "        # Feature Propagation Layers\n",
    "        self.fp_modules = nn.ModuleList()\n",
    "        feature_channels = inter_channels[-1]\n",
    "        for ind in range(num_fp_layers):\n",
    "            fp_module = self._FP_MODULE(in_channels=feature_channels + inter_channels[-2 - ind],\n",
    "                                        mlp_channels=fp_channels[ind],\n",
    "                                        num_neighbors=num_fp_neighbours[ind])\n",
    "            self.fp_modules.append(fp_module)\n",
    "            feature_channels = fp_channels[ind][-1]\n",
    "\n",
    "        # MLP\n",
    "        self.mlp_seg = SharedMLP(feature_channels, seg_channels, ndim=1, dropout_prob=dropout_prob)\n",
    "        self.seg_logit = nn.Conv1d(seg_channels[-1], score_classes, 1, bias=True)\n",
    "\n",
    "#         self.mlp_grasp_eval = SharedMLP(feature_channels + 28, seg_channels, ndim=2, dropout_prob=dropout_prob)\n",
    "#         self.grasp_eval_logit = nn.Conv2d(seg_channels[-1], 1, 1, bias=True)\n",
    "    \n",
    "        self.mlp_R = SharedMLP(feature_channels, seg_channels, ndim=1)\n",
    "        self.R_logit = nn.Conv1d(seg_channels[-1], 4, 1, bias=True)\n",
    "\n",
    "        self.mlp_t = SharedMLP(feature_channels, seg_channels, ndim=1)\n",
    "        self.t_logit = nn.Conv1d(seg_channels[-1], 3, 1, bias=True)\n",
    "\n",
    "        self.mlp_movable = SharedMLP(feature_channels, seg_channels, ndim=1, dropout_prob=dropout_prob)\n",
    "        self.movable_logit = nn.Sequential(\n",
    "            nn.Conv1d(seg_channels[-1], num_removal_directions, 1, bias=True),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "        self.init_weights()\n",
    "    def forward(self, data_batch):\n",
    "        points = data_batch['scene_points']\n",
    "\n",
    "        xyz = points\n",
    "        feature = None\n",
    "\n",
    "        # save intermediate results\n",
    "        inter_xyz = [xyz]\n",
    "        inter_feature = [feature]\n",
    "\n",
    "        # Set Abstraction Layers\n",
    "        for sa_module in self.sa_modules:\n",
    "            xyz, feature = sa_module(xyz, feature)\n",
    "            inter_xyz.append(xyz)\n",
    "            inter_feature.append(feature)\n",
    "\n",
    "        # Feature Propagation Layers\n",
    "        sparse_xyz = xyz\n",
    "        sparse_feature = feature\n",
    "        for fp_ind, fp_module in enumerate(self.fp_modules):\n",
    "            dense_xyz = inter_xyz[-2 - fp_ind]\n",
    "            dense_feature = inter_feature[-2 - fp_ind]\n",
    "            fp_feature = fp_module(dense_xyz, sparse_xyz, dense_feature, sparse_feature)\n",
    "            sparse_xyz = dense_xyz\n",
    "            sparse_feature = fp_feature\n",
    "\n",
    "        # MLP\n",
    "        x = self.mlp_seg(sparse_feature)\n",
    "        logits = self.seg_logit(x)\n",
    "        \n",
    "        R = self.mlp_R(sparse_feature)\n",
    "        R = self.R_logit(R)\n",
    "        # R = toRotMatrix(R)\n",
    "        # R = euler2RotMatrix(R)\n",
    "\n",
    "        t = self.mlp_t(sparse_feature)\n",
    "        t = self.t_logit(t)\n",
    "\n",
    "        # t = points + t\n",
    "\n",
    "        mov = self.mlp_movable(sparse_feature)\n",
    "        mov = self.movable_logit(mov)  # (B, 5, N)\n",
    "        \n",
    "#         local_search_frame = torch.cat([R, t], dim=1).unsqueeze(-1)\n",
    "#         local_search_frame = local_search_frame.repeat(1, 4, 1, 1)\n",
    "#         sparse_feature = sparse_feature.unsqueeze(-1)\n",
    "#         valid_feature = torch.cat([sparse_feature, local_search_frame], dim=1)\n",
    "#         local_search_logit = self.grasp_eval_logit(self.mlp_grasp_eval(valid_feature))\n",
    "            \n",
    "        preds = {\"score\": logits,\n",
    "                 \"frame_R\": R,\n",
    "                 \"frame_t\": t,\n",
    "                 \"movable_logits\": mov,\n",
    "                 }\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def init_weights(self):\n",
    "        # nn_utils.init.zeros_(self.t_logit.weight)\n",
    "        # nn_utils.init.zeros_(self.t_logit.bias)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506caec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNet2Loss(nn.Module):\n",
    "    def __init__(self, label_smoothing=0, neg_weight=0.1):\n",
    "        super(PointNet2Loss, self).__init__()\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.neg_weight = neg_weight\n",
    "\n",
    "    def forward(self, preds, labels):\n",
    "        scene_score_logits = preds[\"scene_score_logits\"]  # (B, C, N2)\n",
    "        score_classes = scene_score_logits.shape[1]\n",
    "        weight = torch.ones(score_classes, device=scene_score_logits.device)\n",
    "        weight[0] = self.neg_weight\n",
    "\n",
    "        movable_logits = preds[\"movable_logits\"]\n",
    "        movable_labels = labels[\"scene_movable_labels\"]\n",
    "        mov_loss = F.l1_loss(movable_logits, movable_labels)\n",
    "        # mov_weight = torch.ones(2, device=movable_logits.device)\n",
    "        # mov_weight[0] = 0.9\n",
    "\n",
    "        scene_score_labels = labels[\"scene_score_labels\"]  # (B, N)\n",
    "\n",
    "        if self.label_smoothing > 0:\n",
    "            selected_logits = scene_score_logits.transpose(1, 2).contiguous().view(-1, score_classes)\n",
    "            scene_score_labels = scene_score_labels.view(-1)\n",
    "            cls_loss = smooth_cross_entropy(selected_logits, scene_score_labels, self.label_smoothing,\n",
    "                                            weight=weight)\n",
    "\n",
    "            # movable_logits = movable_logits.transpose(1, 2).contiguous().view(-1, 2)\n",
    "            # movable_labels = movable_labels.view(-1)\n",
    "            #\n",
    "            # mov_loss = smooth_cross_entropy(movable_logits, movable_labels, self.label_smoothing, weight=mov_weight)\n",
    "        else:\n",
    "            cls_loss = F.cross_entropy(scene_score_logits, scene_score_labels, weight)\n",
    "\n",
    "        gt_frame_R = labels[\"best_frame_R\"]\n",
    "        num_frame_points = gt_frame_R.shape[2]\n",
    "        pred_frame_R = preds[\"frame_R\"][:, :, :num_frame_points]\n",
    "        R_loss_1 = ((pred_frame_R - gt_frame_R) ** 2).mean(1, True)\n",
    "        gt_frame_R_inv = gt_frame_R.clone()\n",
    "        gt_frame_R_inv[:, 1:3, :] = - gt_frame_R_inv[:, 1:3, :]\n",
    "        gt_frame_R_inv[:, 4:6, :] = - gt_frame_R_inv[:, 4:6, :]\n",
    "        gt_frame_R_inv[:, 7:9, :] = - gt_frame_R_inv[:, 7:9, :]\n",
    "        R_loss_2 = ((pred_frame_R - gt_frame_R_inv) ** 2).mean(1, True)\n",
    "        R_loss, _ = torch.min(torch.cat([R_loss_1, R_loss_2], dim=1), dim=1)\n",
    "\n",
    "        # weight loss according to gt_score\n",
    "        gt_scene_score = labels[\"scene_score\"][:, :num_frame_points]\n",
    "        R_loss = (R_loss * gt_scene_score).mean() * 5.0\n",
    "        # gt_norm = torch.stack([gt_frame_R[:, 0, :], gt_frame_R[:, 3, :], gt_frame_R[:, 6, :]], dim=1)\n",
    "        # pred_norm = torch.stack([pred_frame_R[:, 0, :], pred_frame_R[:, 3, :], pred_frame_R[:, 6, :]], dim=1)\n",
    "        # norm_loss = torch.mean((pred_norm - gt_norm) ** 2)\n",
    "\n",
    "        gt_frame_t = labels[\"best_frame_t\"]\n",
    "        pred_frame_t = preds[\"frame_t\"][:, :, :num_frame_points]\n",
    "        # t_loss = torch.mean(((pred_frame_t - gt_frame_t) ** 2).sum(1) * gt_scene_score) * 20.0\n",
    "        t_loss = F.cross_entropy(pred_frame_t, gt_frame_t) * 0.2\n",
    "\n",
    "        loss_dict = {\"cls_loss\": cls_loss,\n",
    "                     \"R_loss\": R_loss,\n",
    "                     # \"norm_loss\": norm_loss,\n",
    "                     \"t_loss\": t_loss,\n",
    "                     \"mov_loss\": mov_loss,\n",
    "                     }\n",
    "\n",
    "        return loss_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa2b6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNet2Metric(nn.Module):\n",
    "    def forward(self, preds, labels):\n",
    "        scene_score_logits = preds[\"scene_score_logits\"]  # (B, C, N2)\n",
    "        score_classes = scene_score_logits.shape[1]\n",
    "\n",
    "        scene_score_labels = labels[\"scene_score_labels\"]  # (B, N)\n",
    "\n",
    "        selected_preds = scene_score_logits.argmax(1).view(-1)\n",
    "        scene_score_labels = scene_score_labels.view(-1)\n",
    "\n",
    "        cls_acc = selected_preds.eq(scene_score_labels).float()\n",
    "\n",
    "        movable_logits = preds[\"movable_logits\"]\n",
    "        movable_labels = labels[\"scene_movable_labels\"]\n",
    "        movable_preds = (movable_logits > 0.5).view(-1).int()\n",
    "        movable_labels = movable_labels.view(-1).int()\n",
    "        mov_acc = movable_preds.eq(movable_labels).float()\n",
    "\n",
    "        gt_frame_R = labels[\"best_frame_R\"]\n",
    "        batch_size, _, num_frame_points = gt_frame_R.shape\n",
    "        pred_frame_R = preds[\"frame_R\"][:, :, :num_frame_points]\n",
    "        gt_frame_R = gt_frame_R.transpose(1, 2).contiguous().view(batch_size * num_frame_points, 3, 3)\n",
    "        gt_frame_R_inv = gt_frame_R.clone()\n",
    "        gt_frame_R_inv[:, :, 1:] = -gt_frame_R_inv[:, :, 1:]\n",
    "        pred_frame_R = pred_frame_R.transpose(1, 2).contiguous().view(batch_size * num_frame_points, 3, 3)\n",
    "        M = torch.bmm(gt_frame_R, pred_frame_R.transpose(1, 2))\n",
    "        angle = torch.acos(torch.clamp((M[:, 0, 0] + M[:, 1, 1] + M[:, 2, 2] - 1.0) / 2.0, -1.0, 1.0))\n",
    "        M_inv = torch.bmm(gt_frame_R_inv, pred_frame_R.transpose(1, 2))\n",
    "        angle_inv = torch.acos(torch.clamp((M_inv[:, 0, 0] + M_inv[:, 1, 1] + M_inv[:, 2, 2] - 1.0) / 2.0, -1.0, 1.0))\n",
    "\n",
    "        angle_min = torch.stack([angle, angle_inv], dim=1).min(1)[0]\n",
    "        gt_scene_score = labels[\"scene_score\"][:, :num_frame_points].contiguous().view(-1)\n",
    "        angle_min = (gt_scene_score * angle_min).mean()\n",
    "\n",
    "        gt_frame_t = labels[\"best_frame_t\"].view(-1)\n",
    "        pred_frame_t = preds[\"frame_t\"][:, :, :num_frame_points]\n",
    "        pred_frame_t = torch.argmax(pred_frame_t, dim=1).view(-1)\n",
    "        t_acc = pred_frame_t.eq(gt_frame_t).float()\n",
    "\n",
    "        # t_err = torch.mean(torch.sqrt(((gt_frame_t - pred_frame_t) ** 2).sum(1)))\n",
    "\n",
    "        return {\"cls_acc\": cls_acc,\n",
    "                \"mov_acc\": mov_acc,\n",
    "                \"R_err\": angle_min,\n",
    "                \"t_acc\": t_acc,\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c773b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_pointnet2_cls(cfg):\n",
    "def build_model(cfg):\n",
    "    net = PointNet2(\n",
    "        score_classes=cfg.DATA.SCORE_CLASSES,\n",
    "        num_centroids=cfg.MODEL.PN2.NUM_CENTROIDS,\n",
    "        radius=cfg.MODEL.PN2.RADIUS,\n",
    "        num_neighbours=cfg.MODEL.PN2.NUM_NEIGHBOURS,\n",
    "        sa_channels=cfg.MODEL.PN2.SA_CHANNELS,\n",
    "        fp_channels=cfg.MODEL.PN2.FP_CHANNELS,\n",
    "        num_fp_neighbours=cfg.MODEL.PN2.NUM_FP_NEIGHBOURS,\n",
    "        seg_channels=cfg.MODEL.PN2.SEG_CHANNELS,\n",
    "        num_removal_directions=cfg.DATA.NUM_REMOVAL_DIRECTIONS,\n",
    "        dropout_prob=cfg.MODEL.PN2.DROPOUT_PROB,\n",
    "    )\n",
    "\n",
    "    loss_func = PointNet2Loss(\n",
    "        label_smoothing=cfg.MODEL.PN2.LABEL_SMOOTHING,\n",
    "        neg_weight=cfg.MODEL.PN2.NEG_WEIGHT,\n",
    "    )\n",
    "    metric = PointNet2Metric()\n",
    "\n",
    "    return net, loss_func, metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706d845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/po/TM5/graspnetAPI'\n",
    "valid_obj_idxs, grasp_labels = load_grasp_labels(root)\n",
    "train_dataset = GraspNetDataset(root, valid_obj_idxs, grasp_labels, split='train', remove_outlier=True, remove_invisible=True, num_points=20000)\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f1e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grasp_list = train_dataset.__getitem__(0)['grasp_list']\n",
    "# point_clouds = train_dataset.__getitem__(0)['point_clouds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce5a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(grasp_list.shape,point_clouds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18838132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# Init datasets and dataloaders \n",
    "def my_worker_init_fn(worker_id):\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c1cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATALOADER = DataLoader(train_dataset, batch_size=2, shuffle=True,\n",
    "    num_workers=4, worker_init_fn=my_worker_init_fn, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e267b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proposal_test.py\n",
    "import sys,os\n",
    "sys.path.append('/home/po/TM5/s4g-release/inference')\n",
    "import numpy as np\n",
    "import open3d\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from grasp_proposal.cloud_processor.cloud_processor import CloudPreProcessor\n",
    "from grasp_proposal.configs.yacs_config import load_cfg_from_file\n",
    "# from grasp_proposal.network_models.models.build_model import build_model\n",
    "from grasp_proposal.utils.checkpoint import CheckPointer\n",
    "from grasp_proposal.utils.file_logger_cls import loggin_to_file\n",
    "from grasp_proposal.utils.grasp_visualizer import GraspVisualizer\n",
    "from grasp_proposal.utils.logger import setup_logger, MetricLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3710fb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-29 22:53:44,001 S4G INFO: Using 2 of GPUs\n",
      "2021-11-29 22:53:44,001 S4G INFO: Load config file from /home/po/TM5/s4g-release/inference/grasp_proposal/configs/curvature_model.yaml\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'build_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3947a7c4f1d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running with config \\n {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'build_model' is not defined"
     ]
    }
   ],
   "source": [
    "# proposal_test.py\n",
    "# load_static batch data\n",
    "# \n",
    "cfg_path = \"/home/po/TM5/s4g-release/inference/grasp_proposal/configs/curvature_model.yaml\"\n",
    "cfg = load_cfg_from_file(cfg_path)\n",
    "cfg.defrost()\n",
    "# cfg.TEST.WEIGHT = cfg.TEST.WEIGHT.replace(\"${PROJECT_HOME}\", os.path.join(os.getcwd(), \"../\"))\n",
    "# cfg.TEST.WEIGHT = '/home/po/TM5/s4g-release/inference/trained_models/curvature_model.pth'\n",
    "cfg.TEST.WEIGHT = cfg.TEST.WEIGHT.replace(\"${PROJECT_HOME}\", os.path.join('/home/po/TM5/s4g-release/inference'))\n",
    "cfg.freeze()\n",
    "assert cfg.TEST.BATCH_SIZE == 1\n",
    "\n",
    "output_dir = \"./output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "logger = setup_logger(\"S4G\", output_dir, \"unit_test\")\n",
    "logger.info(\"Using {} of GPUs\".format(torch.cuda.device_count()))\n",
    "logger.info(\"Load config file from {}\".format(cfg_path))\n",
    "logger.debug(\"Running with config \\n {}\".format(cfg))\n",
    "\n",
    "model, _, _ = build_model(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a4a414",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef6d52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 10:\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "elif torch.cuda.device_count() == 2:\n",
    "    model = model.cuda()\n",
    "\n",
    "trained_model_path = output_dir\n",
    "check_pointer = CheckPointer(model, save_dir=trained_model_path, logger=logger)\n",
    "# if cfg.TEST.WEIGHT:\n",
    "#     weight_path = cfg.TEST.WEIGHT.replace(\"@\", output_dir)\n",
    "#     check_pointer.load(weight_path, resume=False)\n",
    "# else:\n",
    "#     check_pointer.load(None, resume=True)\n",
    "    \n",
    "batch_size = 2\n",
    "num_points = 20000\n",
    "score_classes = 3\n",
    "tmp_dic = next(iter(TRAIN_DATALOADER))\n",
    "point_clouds_tmp = tmp_dic['point_clouds']\n",
    "point_clouds = point_clouds_tmp.permute(0,2,1)\n",
    "\n",
    "grasp_ALL = tmp_dic['grasp_list']\n",
    "\n",
    "data_batch = {\n",
    "    \"scene_points\": point_clouds.float().cuda(),\n",
    "#     \"scene_score_labels\": torch.tensor(scene_score_labels).long().cuda(),\n",
    "#     \"frame_index\": torch.tensor(frame_index).long().cuda()\n",
    "}\n",
    "model.eval()\n",
    "meters = MetricLogger(delimiter=\"  \")\n",
    "pcd = 20000\n",
    "# data_batch, pcd = load_static_data_batch()\n",
    "tic = time.time()\n",
    "with torch.no_grad():\n",
    "    data_batch = {k: v.cuda(non_blocking=True) for k, v in data_batch.items() if isinstance(v, torch.Tensor)}\n",
    "    tac = time.time()\n",
    "    data_time = tac - tic\n",
    "    predictions = model(data_batch)\n",
    "#     tic = time.time()\n",
    "#     batch_time = tic - tac\n",
    "#     with open(\"inference_time_{}.txt\".format(\"ours\"), \"a+\") as f:\n",
    "#         f.write(\"{:.4f}\\n\".format(batch_time * 1000.0))\n",
    "#     meters.update(time=batch_time, data=data_time)\n",
    "\n",
    "#     logger.info(meters.delimiter.join([\"{meters}\", ]).format(meters=str(meters), ))\n",
    "\n",
    "#     top_poses, score = loggin_to_file(data_batch, predictions, 0, output_dir, prefix=\"test\", with_label=False)\n",
    "#     visualizer = GraspVisualizer(pcd)\n",
    "#     visualizer.add_multiple_poses(top_poses)\n",
    "#     visualizer.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be2696",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions['score'].shape,predictions['frame_R'].shape,predictions['frame_t'].shape,predictions['movable_logits'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f69b04d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 10000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "num_points = 10000\n",
    "score_classes = 3\n",
    "scene_points = np.random.randn(batch_size, 3, num_points)\n",
    "scene_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7b3d49b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TRAIN_DATALOADER' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b346021d61fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtmp_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_DATALOADER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mpoint_clouds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'point_clouds'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m frame_index = np.stack(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TRAIN_DATALOADER' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "num_points = 10000\n",
    "score_classes = 3\n",
    "\n",
    "import numpy as np\n",
    "tmp_dic = next(iter(TRAIN_DATALOADER))\n",
    "point_clouds = tmp_dic['point_clouds']\n",
    "frame_index = np.stack(\n",
    "    [np.random.choice(np.arange(num_points), num_frame, replace=False) for _ in range(batch_size)])\n",
    "\n",
    "scene_score_labels = np.random.randint(0, score_classes - 1, (batch_size, num_frame))\n",
    "point_clouds = point_clouds.permute(0,2,1)\n",
    "grasp_ALL = tmp_dic['grasp_list']\n",
    "\n",
    "data_batch = {\n",
    "    \"scene_points\": torch.tensor(scene_points).float().cuda(),\n",
    "    \"scene_score_labels\": torch.tensor(scene_score_labels).long().cuda(),\n",
    "    \"frame_index\": torch.tensor(frame_index).long().cuda()\n",
    "}\n",
    "\n",
    "pn2 = PointNet2(score_classes=score_classes).cuda()\n",
    "pn2\n",
    "print(scene_score_labels.shape)\n",
    "# pn2_loss_fn = PointNet2Loss().cuda()\n",
    "# pn2_metric = PointNet2Metric().cuda()\n",
    "\n",
    "# preds = pn2(data_batch)\n",
    "# print(\"PointNet2: \")\n",
    "# for k, v in preds.items():\n",
    "#     print(k, v.shape)\n",
    "\n",
    "# loss = pn2_loss_fn(preds, data_batch)\n",
    "# print(loss)\n",
    "# metric = pn2_metric(preds, data_batch)\n",
    "# print(metric)\n",
    "\n",
    "# sum(loss.values()).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9174cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c94d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/po/TM5/s4g-release/inference')\n",
    "import numpy as np\n",
    "import open3d\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from grasp_proposal.cloud_processor.cloud_processor import CloudPreProcessor\n",
    "from grasp_proposal.configs.yacs_config import load_cfg_from_file\n",
    "from grasp_proposal.network_models.models.build_model import build_model\n",
    "from grasp_proposal.utils.checkpoint import CheckPointer\n",
    "from grasp_proposal.utils.file_logger_cls import loggin_to_file\n",
    "from grasp_proposal.utils.grasp_visualizer import GraspVisualizer\n",
    "from grasp_proposal.utils.logger import setup_logger, MetricLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcfb31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_training_data = np.load(\"/home/po/TM5/s4g-release/2638_view_0.p\", allow_pickle=True)\n",
    "# cloud_array = single_training_data[\"point_cloud\"]\n",
    "# cloud = CloudPreProcessor(open3d.geometry.PointCloud(open3d.utility.Vector3dVector(cloud_array.T)), False)\n",
    "# # do not filter workspace here since training data\n",
    "# cloud.voxelize()\n",
    "# cloud.remove_outliers()\n",
    "# points = np.asarray(cloud.pcd.points)\n",
    "# if points.shape[0] > 25600:\n",
    "#     random_index = np.random.choice(np.arange(points.shape[0]), 25600, replace=False)\n",
    "# else:\n",
    "#     random_index = np.random.choice(np.arange(points.shape[0]), 25600, replace=True)\n",
    "\n",
    "# points = points[random_index, :]\n",
    "# C = torch.tensor(points, dtype=torch.float32).unsqueeze(0).transpose(1, 2)\n",
    "# B = torch.tensor(points, dtype=torch.float32).unsqueeze(0)\n",
    "# A = torch.tensor(points, dtype=torch.float32)\n",
    "# # data_batch = {\"scene_points\": torch.tensor(points, dtype=torch.float32).unsqueeze(0).transpose(1, 2)}\n",
    "# # print(data_batch,cloud.pcd)\n",
    "# print(A.shape,B.shape,C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48961321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_training_data['point_cloud'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbcd926",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = torch.ones(1, 9, 25600) \n",
    "t = torch.ones(1, 3, 25600)\n",
    "local_search_frame = torch.cat([R, t], dim=1).unsqueeze(-1)\n",
    "local_search_frame = local_search_frame.repeat(1, 4, 1, 1)\n",
    "# sparse_feature = sparse_feature.unsqueeze(-1)\n",
    "# valid_feature = torch.cat([sparse_feature, local_search_frame], dim=1)\n",
    "# local_search_logit = self.grasp_eval_logit(self.mlp_grasp_eval(valid_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c3ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_search_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104840bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_search_frame1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e374cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f97f57b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
