{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acbc1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os, sys\n",
    "import sys\n",
    "from torchsummary import summary\n",
    "# sys.path.append('/home/po/TM5/Pointnet_Pointnet2_pytorch')\n",
    "sys.path.append('/home/po/TM5/s4g-release/inference/grasp_proposal/network_models')\n",
    "sys.path.append('/home/po/TM5/graspnetAPI/graspnetAPI')\n",
    "sys.path.append('/home/po/TM5')\n",
    "from nn_utils.mlp import SharedMLP\n",
    "from pointnet2_utils.modules import PointNetSAModule, PointnetFPModule, PointNetSAAvgModule\n",
    "from nn_utils.functional import smooth_cross_entropy\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc011930",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/po/TM5/graspnet-baseline')\n",
    "import scipy.io as scio\n",
    "from dataset.graspnet_dataset1 import GraspNetDataset, collate_fn, load_grasp_labels\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e84f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNet2(nn.Module):\n",
    "    \"\"\"PointNet++ part segmentation with single-scale grouping\n",
    "\n",
    "    PointNetSA: PointNet Set Abstraction Layer\n",
    "    PointNetFP: PointNet Feature Propagation Layer\n",
    "\n",
    "    Args:\n",
    "        score_classes (int): the number of grasp score classes\n",
    "        num_centroids (tuple of int): the numbers of centroids to sample in each set abstraction module\n",
    "        radius (tuple of float): a tuple of radius to query neighbours in each set abstraction module\n",
    "        num_neighbours (tuple of int): the numbers of neighbours to query for each centroid\n",
    "        sa_channels (tuple of tuple of int): the numbers of channels within each set abstraction module\n",
    "        fp_channels (tuple of tuple of int): the numbers of channels for feature propagation (FP) module\n",
    "        num_fp_neighbours (tuple of int): the numbers of nearest neighbor used in FP\n",
    "        seg_channels (tuple of int): the numbers of channels in segmentation mlp\n",
    "        dropout_prob (float): the probability to dropout input features\n",
    "\n",
    "    References:\n",
    "        https://github.com/charlesq34/pointnet2/blob/master/models/pointnet2_part_seg.py\n",
    "\n",
    "    \"\"\"\n",
    "    _SA_MODULE = PointNetSAModule\n",
    "    _FP_MODULE = PointnetFPModule\n",
    "\n",
    "    def __init__(self,\n",
    "                 score_classes,\n",
    "                 num_centroids=(10240, 1024, 128, 0),\n",
    "                 radius=(0.2, 0.3, 0.4, -1.0),\n",
    "                 num_neighbours=(64, 64, 64, -1),\n",
    "                 sa_channels=((32, 32, 64), (64, 64, 128), (128, 128, 256), (256, 512, 1024)),\n",
    "                 fp_channels=((256, 256), (256, 128), (128, 128), (64, 64, 64)),\n",
    "                 num_fp_neighbours=(0, 3, 3, 3),\n",
    "                 seg_channels=(128,),\n",
    "                 num_removal_directions=5,\n",
    "                 dropout_prob=0.5):\n",
    "        super(PointNet2, self).__init__()\n",
    "\n",
    "        # Sanity check\n",
    "        num_sa_layers = len(num_centroids)\n",
    "        num_fp_layers = len(fp_channels)\n",
    "        assert len(radius) == num_sa_layers\n",
    "        assert len(num_neighbours) == num_sa_layers\n",
    "        assert len(sa_channels) == num_sa_layers\n",
    "        assert num_sa_layers == num_fp_layers\n",
    "        assert len(num_fp_neighbours) == num_fp_layers\n",
    "\n",
    "        # Set Abstraction Layers\n",
    "        feature_channels = 0\n",
    "        self.sa_modules = nn.ModuleList()\n",
    "        for ind in range(num_sa_layers):\n",
    "            sa_module = self._SA_MODULE(in_channels=feature_channels,\n",
    "                                        mlp_channels=sa_channels[ind],\n",
    "                                        num_centroids=num_centroids[ind],\n",
    "                                        radius=radius[ind],\n",
    "                                        num_neighbours=num_neighbours[ind],\n",
    "                                        use_xyz=True)\n",
    "            self.sa_modules.append(sa_module)\n",
    "            feature_channels = sa_channels[ind][-1]\n",
    "\n",
    "        inter_channels = [0]\n",
    "        inter_channels.extend([x[-1] for x in sa_channels])\n",
    "\n",
    "        # Feature Propagation Layers\n",
    "        self.fp_modules = nn.ModuleList()\n",
    "        feature_channels = inter_channels[-1]\n",
    "        for ind in range(num_fp_layers):\n",
    "            fp_module = self._FP_MODULE(in_channels=feature_channels + inter_channels[-2 - ind],\n",
    "                                        mlp_channels=fp_channels[ind],\n",
    "                                        num_neighbors=num_fp_neighbours[ind])\n",
    "            self.fp_modules.append(fp_module)\n",
    "            feature_channels = fp_channels[ind][-1]\n",
    "\n",
    "        # MLP\n",
    "#         self.mlp_seg = SharedMLP(feature_channels, seg_channels, ndim=1, dropout_prob=dropout_prob)\n",
    "#         self.seg_logit = nn.Conv1d(seg_channels[-1], score_classes, 1, bias=True)\n",
    "\n",
    "        self.mlp_grasp_eval = SharedMLP(feature_channels + 28, seg_channels, ndim=2, dropout_prob=dropout_prob)\n",
    "        self.grasp_eval_logit = nn.Conv2d(seg_channels[-1], 1, 1, bias=True)\n",
    "    \n",
    "        self.mlp_R = SharedMLP(feature_channels, seg_channels, ndim=1)\n",
    "        self.R_logit = nn.Conv1d(seg_channels[-1], 4, 1, bias=True)\n",
    "\n",
    "        self.mlp_t = SharedMLP(feature_channels, seg_channels, ndim=1)\n",
    "        self.t_logit = nn.Conv1d(seg_channels[-1], 3, 1, bias=True)\n",
    "\n",
    "#         self.mlp_movable = SharedMLP(feature_channels, seg_channels, ndim=1, dropout_prob=dropout_prob)\n",
    "#         self.movable_logit = nn.Sequential(\n",
    "#             nn.Conv1d(seg_channels[-1], num_removal_directions, 1, bias=True),\n",
    "#             nn.Sigmoid())\n",
    "\n",
    "        self.init_weights()\n",
    "    def forward(self, data_batch):\n",
    "        points = data_batch['point_clouds']\n",
    "\n",
    "        xyz = points\n",
    "        feature = None\n",
    "\n",
    "        # save intermediate results\n",
    "        inter_xyz = [xyz]\n",
    "        inter_feature = [feature]\n",
    "\n",
    "        # Set Abstraction Layers\n",
    "        for sa_module in self.sa_modules:\n",
    "            xyz, feature = sa_module(xyz, feature)\n",
    "            inter_xyz.append(xyz)\n",
    "            inter_feature.append(feature)\n",
    "\n",
    "        # Feature Propagation Layers\n",
    "        sparse_xyz = xyz\n",
    "        sparse_feature = feature\n",
    "        for fp_ind, fp_module in enumerate(self.fp_modules):\n",
    "            dense_xyz = inter_xyz[-2 - fp_ind]\n",
    "            dense_feature = inter_feature[-2 - fp_ind]\n",
    "            fp_feature = fp_module(dense_xyz, sparse_xyz, dense_feature, sparse_feature)\n",
    "            sparse_xyz = dense_xyz\n",
    "            sparse_feature = fp_feature\n",
    "\n",
    "        # MLP\n",
    "#         x = self.mlp_seg(sparse_feature)\n",
    "#         logits = self.seg_logit(x)\n",
    "        \n",
    "              \n",
    "        \n",
    "        \n",
    "        R = self.mlp_R(sparse_feature)\n",
    "        R = self.R_logit(R)\n",
    "        R = F.normalize(R, dim=1)\n",
    "        # R = toRotMatrix(R)\n",
    "        # R = euler2RotMatrix(R)\n",
    "\n",
    "        t = self.mlp_t(sparse_feature)\n",
    "        t = self.t_logit(t)\n",
    "        \n",
    "        local_search_frame = torch.cat([R, t], dim=1).unsqueeze(-1)\n",
    "        local_search_frame = local_search_frame.repeat(1, 4, 1, 1)\n",
    "        sparse_feature = sparse_feature.unsqueeze(-1)\n",
    "        valid_feature = torch.cat([sparse_feature, local_search_frame], dim=1)\n",
    "        local_search_logit = self.grasp_eval_logit(self.mlp_grasp_eval(valid_feature))\n",
    "        # t = points + t\n",
    "        \n",
    "#         mov = self.mlp_movable(sparse_feature)\n",
    "#         mov = self.movable_logit(mov)  # (B, 5, N)\n",
    "        \n",
    "            \n",
    "        preds = {\n",
    "#                 \"score\": logits,\n",
    "                \"score_pred\": local_search_logit,\n",
    "                 \"q_pred\": R,\n",
    "                 \"xyz_pred\": t,\n",
    "#                  \"movable_logits\": mov,\n",
    "                 }\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def init_weights(self):\n",
    "        # nn_utils.init.zeros_(self.t_logit.weight)\n",
    "        # nn_utils.init.zeros_(self.t_logit.bias)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e402cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNet2Loss(nn.Module):\n",
    "    def __init__(self, label_smoothing=0, neg_weight=0.1):\n",
    "        super(PointNet2Loss, self).__init__()\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.neg_weight = neg_weight\n",
    "\n",
    "    def forward(self, preds, labels):\n",
    "        \n",
    "\n",
    "        q_label = labels[\"q_label\"].permute(0,2,1)#(1,4,20)\n",
    "        q_pred = preds[\"q_pred\"]\n",
    "        q_loss = ((q_pred - q_label) ** 2).mean(1, True)\n",
    "        \n",
    "\n",
    "        # weight loss according to gt_score\n",
    "        score_label = labels[\"score_label\"].permute(0,2,1).unsqueeze(-1)\n",
    "        score_pred = preds[\"score_pred\"]\n",
    "        score_loss = ((score_pred - score_label) ** 2).mean(1, True)\n",
    "        \n",
    "\n",
    "        xyz_label = labels[\"xyz_label\"].permute(0,2,1)\n",
    "        xyz_pred = preds[\"xyz_pred\"]\n",
    "        xyz_loss = ((xyz_pred - xyz_label) ** 2).mean(1, True)\n",
    "        \n",
    "    \n",
    "\n",
    "        loss_dict = {\n",
    "                    \"score_loss\": score_loss,\n",
    "                     \"q_loss\": q_loss,\n",
    "                     \"xyz_loss\": xyz_loss,\n",
    "                     }\n",
    "\n",
    "        return loss_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61021ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNet2Metric(nn.Module):\n",
    "    def forward(self, preds, labels):\n",
    "        scene_score_logits = preds[\"scene_score_logits\"]  # (B, C, N2)\n",
    "        score_classes = scene_score_logits.shape[1]\n",
    "\n",
    "        scene_score_labels = labels[\"scene_score_labels\"]  # (B, N)\n",
    "\n",
    "        selected_preds = scene_score_logits.argmax(1).view(-1)\n",
    "        scene_score_labels = scene_score_labels.view(-1)\n",
    "\n",
    "        cls_acc = selected_preds.eq(scene_score_labels).float()\n",
    "\n",
    "        movable_logits = preds[\"movable_logits\"]\n",
    "        movable_labels = labels[\"scene_movable_labels\"]\n",
    "        movable_preds = (movable_logits > 0.5).view(-1).int()\n",
    "        movable_labels = movable_labels.view(-1).int()\n",
    "        mov_acc = movable_preds.eq(movable_labels).float()\n",
    "\n",
    "        gt_frame_R = labels[\"best_frame_R\"]\n",
    "        batch_size, _, num_frame_points = gt_frame_R.shape\n",
    "        pred_frame_R = preds[\"frame_R\"][:, :, :num_frame_points]\n",
    "        gt_frame_R = gt_frame_R.transpose(1, 2).contiguous().view(batch_size * num_frame_points, 3, 3)\n",
    "        gt_frame_R_inv = gt_frame_R.clone()\n",
    "        gt_frame_R_inv[:, :, 1:] = -gt_frame_R_inv[:, :, 1:]\n",
    "        pred_frame_R = pred_frame_R.transpose(1, 2).contiguous().view(batch_size * num_frame_points, 3, 3)\n",
    "        M = torch.bmm(gt_frame_R, pred_frame_R.transpose(1, 2))\n",
    "        angle = torch.acos(torch.clamp((M[:, 0, 0] + M[:, 1, 1] + M[:, 2, 2] - 1.0) / 2.0, -1.0, 1.0))\n",
    "        M_inv = torch.bmm(gt_frame_R_inv, pred_frame_R.transpose(1, 2))\n",
    "        angle_inv = torch.acos(torch.clamp((M_inv[:, 0, 0] + M_inv[:, 1, 1] + M_inv[:, 2, 2] - 1.0) / 2.0, -1.0, 1.0))\n",
    "\n",
    "        angle_min = torch.stack([angle, angle_inv], dim=1).min(1)[0]\n",
    "        gt_scene_score = labels[\"scene_score\"][:, :num_frame_points].contiguous().view(-1)\n",
    "        angle_min = (gt_scene_score * angle_min).mean()\n",
    "\n",
    "        gt_frame_t = labels[\"best_frame_t\"].view(-1)\n",
    "        pred_frame_t = preds[\"frame_t\"][:, :, :num_frame_points]\n",
    "        pred_frame_t = torch.argmax(pred_frame_t, dim=1).view(-1)\n",
    "        t_acc = pred_frame_t.eq(gt_frame_t).float()\n",
    "\n",
    "        # t_err = torch.mean(torch.sqrt(((gt_frame_t - pred_frame_t) ** 2).sum(1)))\n",
    "\n",
    "        return {\"cls_acc\": cls_acc,\n",
    "                \"mov_acc\": mov_acc,\n",
    "                \"R_err\": angle_min,\n",
    "                \"t_acc\": t_acc,\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb45246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_pointnet2_cls(cfg):\n",
    "def build_model(cfg):\n",
    "    net = PointNet2(\n",
    "        score_classes=cfg.DATA.SCORE_CLASSES,\n",
    "        num_centroids=cfg.MODEL.PN2.NUM_CENTROIDS,\n",
    "        radius=cfg.MODEL.PN2.RADIUS,\n",
    "        num_neighbours=cfg.MODEL.PN2.NUM_NEIGHBOURS,\n",
    "        sa_channels=cfg.MODEL.PN2.SA_CHANNELS,\n",
    "        fp_channels=cfg.MODEL.PN2.FP_CHANNELS,\n",
    "        num_fp_neighbours=cfg.MODEL.PN2.NUM_FP_NEIGHBOURS,\n",
    "        seg_channels=cfg.MODEL.PN2.SEG_CHANNELS,\n",
    "        num_removal_directions=cfg.DATA.NUM_REMOVAL_DIRECTIONS,\n",
    "        dropout_prob=cfg.MODEL.PN2.DROPOUT_PROB,\n",
    "    )\n",
    "\n",
    "    loss_func = PointNet2Loss(\n",
    "        label_smoothing=cfg.MODEL.PN2.LABEL_SMOOTHING,\n",
    "        neg_weight=cfg.MODEL.PN2.NEG_WEIGHT,\n",
    "    )\n",
    "    metric = PointNet2Metric()\n",
    "\n",
    "    return net, loss_func, metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0c9769",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/po/TM5/graspnetAPI'\n",
    "valid_obj_idxs, grasp_labels = load_grasp_labels(root)\n",
    "train_dataset = GraspNetDataset(root, valid_obj_idxs, grasp_labels, split='train', remove_outlier=True, remove_invisible=True, num_points=20000)\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b7bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grasp_list = train_dataset.__getitem__(0)['grasp_list']\n",
    "# point_clouds = train_dataset.__getitem__(0)['point_clouds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abda3236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(grasp_list.shape,point_clouds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00672e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# Init datasets and dataloaders \n",
    "def my_worker_init_fn(worker_id):\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1704b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATALOADER = DataLoader(train_dataset, batch_size=1, shuffle=True,\n",
    "    num_workers=4, worker_init_fn=my_worker_init_fn, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5273e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1101f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proposal_test.py\n",
    "import sys,os\n",
    "sys.path.append('/home/po/TM5/s4g-release/inference')\n",
    "import numpy as np\n",
    "import open3d\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from grasp_proposal.cloud_processor.cloud_processor import CloudPreProcessor\n",
    "from grasp_proposal.configs.yacs_config import load_cfg_from_file\n",
    "# from grasp_proposal.network_models.models.build_model import build_model\n",
    "from grasp_proposal.utils.checkpoint import CheckPointer\n",
    "from grasp_proposal.utils.file_logger_cls import loggin_to_file\n",
    "from grasp_proposal.utils.grasp_visualizer import GraspVisualizer\n",
    "from grasp_proposal.utils.logger import setup_logger, MetricLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b305c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proposal_test.py\n",
    "# load_static batch data\n",
    "# \n",
    "cfg_path = \"/home/po/TM5/s4g-release/inference/grasp_proposal/configs/curvature_model.yaml\"\n",
    "cfg = load_cfg_from_file(cfg_path)\n",
    "cfg.defrost()\n",
    "# cfg.TEST.WEIGHT = cfg.TEST.WEIGHT.replace(\"${PROJECT_HOME}\", os.path.join(os.getcwd(), \"../\"))\n",
    "# cfg.TEST.WEIGHT = '/home/po/TM5/s4g-release/inference/trained_models/curvature_model.pth'\n",
    "cfg.TEST.WEIGHT = cfg.TEST.WEIGHT.replace(\"${PROJECT_HOME}\", os.path.join('/home/po/TM5/s4g-release/inference'))\n",
    "cfg.freeze()\n",
    "assert cfg.TEST.BATCH_SIZE == 1\n",
    "\n",
    "output_dir = \"./output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "logger = setup_logger(\"S4G\", output_dir, \"unit_test\")\n",
    "logger.info(\"Using {} of GPUs\".format(torch.cuda.device_count()))\n",
    "logger.info(\"Load config file from {}\".format(cfg_path))\n",
    "logger.debug(\"Running with config \\n {}\".format(cfg))\n",
    "\n",
    "model, loss, _ = build_model(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5579219",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "model.train()\n",
    "for batch_idx, batch_data_label in enumerate(TRAIN_DATALOADER):\n",
    "    data_batch = {\n",
    "    \"point_clouds\": batch_data_label['point_clouds'].float().cuda(),\n",
    "    \"xyz_label\": batch_data_label['xyz_label'].float().cuda(),\n",
    "    \"q_label\": batch_data_label['q_label'].float().cuda(),\n",
    "    \"score_label\": batch_data_label['score_label'].float().cuda(),\n",
    "    }\n",
    "    predictions = model(data_batch)\n",
    "    print(predictions)\n",
    "\n",
    "    loss = loss(predictions, data_batch).cuda()\n",
    "    sum(loss.values()).backward()   \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(loss['xyz_loss'],loss['q_loss'],loss['score_loss'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
