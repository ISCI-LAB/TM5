{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17c96e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/po/TM5/graspnet-baseline/dataset'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" GraspNet dataset processing.\n",
    "    Author: chenxi-wang\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import collections.abc as container_abcs\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "BASE_DIR = os.path.dirname('/home/po/TM5/graspnet-baseline/dataset/graspnet_dataset.py')\n",
    "ROOT_DIR = BASE_DIR\n",
    "sys.path.append(os.path.join('/home/po/TM5/graspnet-baseline', 'utils'))\n",
    "from data_utils import CameraInfo, transform_point_cloud, create_point_cloud_from_depth_image,\\\n",
    "                            get_workspace_mask, remove_invisible_grasp_points\n",
    "BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d34e54e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 2021-11-25 04:41:28,529 - rigid_transformations - Failed to import geometry msgs in rigid_transformations.py.\n",
      "WARNING - 2021-11-25 04:41:28,530 - rigid_transformations - Failed to import ros dependencies in rigid_transforms.py\n",
      "WARNING - 2021-11-25 04:41:28,530 - rigid_transformations - autolab_core not installed as catkin package, RigidTransform ros methods will be unavailable\n"
     ]
    }
   ],
   "source": [
    "#graspnetAPI graspnet.py API interface Path\n",
    "sys.path.append('/home/po/TM5/graspnetAPI/graspnetAPI')\n",
    "sys.path.append('/home/po/TM5/graspnetAPI/graspnetAPI/utils')\n",
    "from graspnetAPI.grasp import Grasp, GraspGroup, RectGrasp, RectGraspGroup, RECT_GRASP_ARRAY_LEN\n",
    "from graspnetAPI.utils.utils import transform_points, parse_posevector\n",
    "from graspnetAPI.utils.xmlhandler import xmlReader\n",
    "\n",
    "TOTAL_SCENE_NUM = 25 #190  ori\n",
    "GRASP_HEIGHT = 0.02\n",
    "\n",
    "from pyquaternion import Quaternion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f182eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading grasping labels...: 100%|██████████| 88/88 [00:34<00:00,  2.52it/s]\n",
      "Loading data path and collision labels...: 100%|██████████| 100/100 [00:42<00:00,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class GraspNetDataset(Dataset):\n",
    "    def __init__(self, root, valid_obj_idxs, grasp_labels, camera='realsense', split='train', num_points=10000,\n",
    "                 remove_outlier=False, remove_invisible=True, augment=False, load_label=True):\n",
    "        assert(num_points<=50000)\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self.num_points = num_points\n",
    "        self.remove_outlier = remove_outlier\n",
    "        self.remove_invisible = remove_invisible\n",
    "        self.valid_obj_idxs = valid_obj_idxs\n",
    "        self.grasp_labels = grasp_labels\n",
    "        self.camera = camera\n",
    "        self.augment = augment\n",
    "        self.load_label = load_label\n",
    "        self.collision_labels = {}\n",
    "#         self.template_grasp = np.zeros([20000,7])\n",
    "        if split == 'train':\n",
    "            self.sceneIds = list( range(100) )\n",
    "        elif split == 'test':\n",
    "            self.sceneIds = list( range(100,190) )\n",
    "        elif split == 'test_seen':\n",
    "            self.sceneIds = list( range(100,130) )\n",
    "        elif split == 'test_similar':\n",
    "            self.sceneIds = list( range(130,160) )\n",
    "        elif split == 'test_novel':\n",
    "            self.sceneIds = list( range(160,190) )\n",
    "        self.sceneIds = ['scene_{}'.format(str(x).zfill(4)) for x in self.sceneIds]\n",
    "        \n",
    "        self.colorpath = []\n",
    "        self.depthpath = []\n",
    "        self.labelpath = []\n",
    "        self.metapath = []\n",
    "        self.scenename = []\n",
    "        self.frameid = []\n",
    "        for x in tqdm(self.sceneIds, desc = 'Loading data path and collision labels...'):\n",
    "            for img_num in range(256):\n",
    "                self.colorpath.append(os.path.join(root, 'scenes', x, camera, 'rgb', str(img_num).zfill(4)+'.png'))\n",
    "                self.depthpath.append(os.path.join(root, 'scenes', x, camera, 'depth', str(img_num).zfill(4)+'.png'))\n",
    "                self.labelpath.append(os.path.join(root, 'scenes', x, camera, 'label', str(img_num).zfill(4)+'.png'))\n",
    "                self.metapath.append(os.path.join(root, 'scenes', x, camera, 'meta', str(img_num).zfill(4)+'.mat'))\n",
    "                self.scenename.append(x.strip())\n",
    "                self.frameid.append(img_num)\n",
    "            if self.load_label:\n",
    "                collision_labels = np.load(os.path.join(root, 'collision_label', x.strip(),  'collision_labels.npz'))\n",
    "                self.collision_labels[x.strip()] = {}\n",
    "                for i in range(len(collision_labels)):\n",
    "                    self.collision_labels[x.strip()][i] = collision_labels['arr_{}'.format(i)]\n",
    "\n",
    "    def scene_list(self):\n",
    "        return self.scenename\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.depthpath)\n",
    "\n",
    "    def augment_data(self, point_clouds, object_poses_list):\n",
    "        # Flipping along the YZ plane\n",
    "        if np.random.random() > 0.5:\n",
    "            flip_mat = np.array([[-1, 0, 0],\n",
    "                                [ 0, 1, 0],\n",
    "                                [ 0, 0, 1]])\n",
    "            point_clouds = transform_point_cloud(point_clouds, flip_mat, '3x3')\n",
    "            for i in range(len(object_poses_list)):\n",
    "                object_poses_list[i] = np.dot(flip_mat, object_poses_list[i]).astype(np.float32)\n",
    "\n",
    "        # Rotation along up-axis/Z-axis\n",
    "        rot_angle = (np.random.random()*np.pi/3) - np.pi/6 # -30 ~ +30 degree\n",
    "        c, s = np.cos(rot_angle), np.sin(rot_angle)\n",
    "        rot_mat = np.array([[1, 0, 0],\n",
    "                            [0, c,-s],\n",
    "                            [0, s, c]])\n",
    "        point_clouds = transform_point_cloud(point_clouds, rot_mat, '3x3')\n",
    "        for i in range(len(object_poses_list)):\n",
    "            object_poses_list[i] = np.dot(rot_mat, object_poses_list[i]).astype(np.float32)\n",
    "\n",
    "        return point_clouds, object_poses_list\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.load_label:\n",
    "            return self.get_data_label(index)\n",
    "        else:\n",
    "            return self.get_data(index)\n",
    "    def loadGrasp(self,index ,format = '6d', camera='realsense', grasp_labels = None, collision_labels = None, fric_coef_thresh=0.4):\n",
    "        '''\n",
    "        **Input:**\n",
    "\n",
    "        - sceneId: int of scene id.\n",
    "\n",
    "        - annId: int of annotation id.\n",
    "\n",
    "        - format: string of grasp format, '6d' or 'rect'.\n",
    "\n",
    "        - camera: string of camera type, 'kinect' or 'realsense'.\n",
    "\n",
    "        - grasp_labels: dict of grasp labels. Call self.loadGraspLabels if not given.\n",
    "\n",
    "        - collision_labels: dict of collision labels. Call self.loadCollisionLabels if not given.\n",
    "\n",
    "        - fric_coef_thresh: float of the frcition coefficient threshold of the grasp. \n",
    "\n",
    "        **ATTENTION**\n",
    "\n",
    "        the LOWER the friction coefficient is, the better the grasp is.\n",
    "\n",
    "        **Output:**\n",
    "\n",
    "        - If format == '6d', return a GraspGroup instance.\n",
    "\n",
    "        - If format == 'rect', return a RectGraspGroup instance.\n",
    "        '''\n",
    "        sceneId = int(self.scenename[index][6:])\n",
    "        annId = self.frameid[index]\n",
    "        grasp_labels = self.grasp_labels\n",
    "        collision_labels = self.collision_labels\n",
    "        import numpy as np\n",
    "        assert format == '6d' or format == 'rect', 'format must be \"6d\" or \"rect\"'\n",
    "        if format == '6d':\n",
    "            from graspnetAPI.utils.xmlhandler import xmlReader\n",
    "            from graspnetAPI.utils.utils import get_obj_pose_list, generate_views, get_model_grasps, transform_points\n",
    "            from graspnetAPI.utils.rotation import batch_viewpoint_params_to_matrix\n",
    "            \n",
    "            camera_poses = np.load(os.path.join(self.root,'scenes','scene_%04d' %(sceneId,),camera, 'camera_poses.npy'))\n",
    "            camera_pose = camera_poses[annId]\n",
    "            scene_reader = xmlReader(os.path.join(self.root,'scenes','scene_%04d' %(sceneId,),camera,'annotations','%04d.xml' %(annId,)))\n",
    "            pose_vectors = scene_reader.getposevectorlist()\n",
    "\n",
    "            obj_list,pose_list = get_obj_pose_list(camera_pose,pose_vectors)\n",
    "            if grasp_labels is None:\n",
    "                print('warning: grasp_labels are not given, calling self.loadGraspLabels to retrieve them')\n",
    "                grasp_labels = self.loadGraspLabels(objIds = obj_list)\n",
    "            if collision_labels is None:\n",
    "                print('warning: collision_labels are not given, calling self.loadCollisionLabels to retrieve them')\n",
    "                collision_labels = self.loadCollisionLabels(sceneId)\n",
    "\n",
    "            num_views, num_angles, num_depths = 300, 12, 4\n",
    "            template_views = generate_views(num_views)\n",
    "            template_views = template_views[np.newaxis, :, np.newaxis, np.newaxis, :]\n",
    "            template_views = np.tile(template_views, [1, 1, num_angles, num_depths, 1])\n",
    "\n",
    "            collision_dump = collision_labels['scene_'+str(sceneId).zfill(4)]\n",
    "\n",
    "            # grasp = dict()\n",
    "            grasp_group = GraspGroup()\n",
    "            for i, (obj_idx, trans) in enumerate(zip(obj_list, pose_list)):\n",
    "                sampled_points, offsets, fric_coefs = grasp_labels[obj_idx+1]\n",
    "                collision = collision_dump[i]\n",
    "                point_inds = np.arange(sampled_points.shape[0])\n",
    "\n",
    "                num_points = len(point_inds)\n",
    "                target_points = sampled_points[:, np.newaxis, np.newaxis, np.newaxis, :]\n",
    "                target_points = np.tile(target_points, [1, num_views, num_angles, num_depths, 1])\n",
    "                views = np.tile(template_views, [num_points, 1, 1, 1, 1])\n",
    "                angles = offsets[:, :, :, :, 0]\n",
    "                depths = offsets[:, :, :, :, 1]\n",
    "                widths = offsets[:, :, :, :, 2]\n",
    "\n",
    "                mask1 = ((fric_coefs <= fric_coef_thresh) & (fric_coefs > 0) & ~collision)\n",
    "                target_points = target_points[mask1]\n",
    "                target_points = transform_points(target_points, trans)\n",
    "                target_points = transform_points(target_points, np.linalg.inv(camera_pose))\n",
    "                views = views[mask1]\n",
    "                angles = angles[mask1]\n",
    "                depths = depths[mask1]\n",
    "                widths = widths[mask1]\n",
    "                fric_coefs = fric_coefs[mask1]\n",
    "\n",
    "                Rs = batch_viewpoint_params_to_matrix(-views, angles)\n",
    "                Rs = np.matmul(trans[np.newaxis, :3, :3], Rs)\n",
    "                Rs = np.matmul(np.linalg.inv(camera_pose)[np.newaxis,:3,:3], Rs)\n",
    "\n",
    "                num_grasp = widths.shape[0]\n",
    "                scores = (1.1 - fric_coefs).reshape(-1,1)\n",
    "                widths = widths.reshape(-1,1)\n",
    "                heights = GRASP_HEIGHT * np.ones((num_grasp,1))\n",
    "                depths = depths.reshape(-1,1)\n",
    "                rotations = Rs.reshape((-1,9))\n",
    "                object_ids = obj_idx * np.ones((num_grasp,1), dtype=np.int32)\n",
    "\n",
    "                obj_grasp_array = np.hstack([scores, widths, heights, depths, rotations, target_points, object_ids]).astype(np.float32)\n",
    "\n",
    "                grasp_group.grasp_group_array = np.concatenate((grasp_group.grasp_group_array, obj_grasp_array))\n",
    "            return grasp_group\n",
    "        else:\n",
    "            # 'rect'\n",
    "            rect_grasps = RectGraspGroup(os.path.join(self.root,'scenes','scene_%04d' % sceneId,camera,'rect','%04d.npy' % annId))\n",
    "            return rect_grasps\n",
    "\n",
    "    def get_data(self, index, return_raw_cloud=False):\n",
    "        color = np.array(Image.open(self.colorpath[index]), dtype=np.float32) / 255.0\n",
    "        depth = np.array(Image.open(self.depthpath[index]))\n",
    "        seg = np.array(Image.open(self.labelpath[index]))\n",
    "        meta = scio.loadmat(self.metapath[index])\n",
    "        scene = self.scenename[index]\n",
    "        try:\n",
    "            intrinsic = meta['intrinsic_matrix']\n",
    "            factor_depth = meta['factor_depth']\n",
    "        except Exception as e:\n",
    "            print(repr(e))\n",
    "            print(scene)\n",
    "        camera = CameraInfo(1280.0, 720.0, intrinsic[0][0], intrinsic[1][1], intrinsic[0][2], intrinsic[1][2], factor_depth)\n",
    "\n",
    "        # generate cloud\n",
    "        cloud = create_point_cloud_from_depth_image(depth, camera, organized=True)\n",
    "\n",
    "        # get valid points\n",
    "        depth_mask = (depth > 0)\n",
    "        seg_mask = (seg > 0)\n",
    "        if self.remove_outlier:\n",
    "            camera_poses = np.load(os.path.join(self.root, 'scenes', scene, self.camera, 'camera_poses.npy'))\n",
    "            align_mat = np.load(os.path.join(self.root, 'scenes', scene, self.camera, 'cam0_wrt_table.npy'))\n",
    "            trans = np.dot(align_mat, camera_poses[self.frameid[index]])\n",
    "            workspace_mask = get_workspace_mask(cloud, seg, trans=trans, organized=True, outlier=0.02)\n",
    "            mask = (depth_mask & workspace_mask)\n",
    "        else:\n",
    "            mask = depth_mask\n",
    "        cloud_masked = cloud[mask]\n",
    "        color_masked = color[mask]\n",
    "        seg_masked = seg[mask]\n",
    "        if return_raw_cloud:\n",
    "            return cloud_masked, color_masked\n",
    "\n",
    "        # sample points\n",
    "        if len(cloud_masked) >= self.num_points:\n",
    "            idxs = np.random.choice(len(cloud_masked), self.num_points, replace=False)\n",
    "        else:\n",
    "            idxs1 = np.arange(len(cloud_masked))\n",
    "            idxs2 = np.random.choice(len(cloud_masked), self.num_points-len(cloud_masked), replace=True)\n",
    "            idxs = np.concatenate([idxs1, idxs2], axis=0)\n",
    "        cloud_sampled = cloud_masked[idxs]\n",
    "        color_sampled = color_masked[idxs]\n",
    "        \n",
    "        ret_dict = {}\n",
    "        ret_dict['point_clouds'] = cloud_sampled.astype(np.float32)\n",
    "        ret_dict['cloud_colors'] = color_sampled.astype(np.float32)\n",
    "\n",
    "        return ret_dict\n",
    "    \n",
    "    def get_data_label(self, index):\n",
    "        color = np.array(Image.open(self.colorpath[index]), dtype=np.float32) / 255.0\n",
    "        depth = np.array(Image.open(self.depthpath[index]))\n",
    "        seg = np.array(Image.open(self.labelpath[index]))\n",
    "        meta = scio.loadmat(self.metapath[index])\n",
    "        scene = self.scenename[index]\n",
    "        try:\n",
    "            obj_idxs = meta['cls_indexes'].flatten().astype(np.int32)\n",
    "            poses = meta['poses']\n",
    "            intrinsic = meta['intrinsic_matrix']\n",
    "            factor_depth = meta['factor_depth']\n",
    "        except Exception as e:\n",
    "            print(repr(e))\n",
    "            print(scene)\n",
    "        camera = CameraInfo(1280.0, 720.0, intrinsic[0][0], intrinsic[1][1], intrinsic[0][2], intrinsic[1][2], factor_depth)\n",
    "\n",
    "        # generate cloud\n",
    "        cloud = create_point_cloud_from_depth_image(depth, camera, organized=True)\n",
    "\n",
    "        # get valid points\n",
    "        depth_mask = (depth > 0)\n",
    "        seg_mask = (seg > 0)\n",
    "        if self.remove_outlier:\n",
    "            camera_poses = np.load(os.path.join(self.root, 'scenes', scene, self.camera, 'camera_poses.npy'))\n",
    "            align_mat = np.load(os.path.join(self.root, 'scenes', scene, self.camera, 'cam0_wrt_table.npy'))\n",
    "            trans = np.dot(align_mat, camera_poses[self.frameid[index]])\n",
    "            workspace_mask = get_workspace_mask(cloud, seg, trans=trans, organized=True, outlier=0.02)\n",
    "            mask = (depth_mask & workspace_mask)\n",
    "        else:\n",
    "            mask = depth_mask\n",
    "        cloud_masked = cloud[mask]\n",
    "        color_masked = color[mask]\n",
    "        seg_masked = seg[mask]\n",
    "\n",
    "        # sample points\n",
    "        if len(cloud_masked) >= self.num_points:\n",
    "            idxs = np.random.choice(len(cloud_masked), self.num_points, replace=False)\n",
    "        else:\n",
    "            idxs1 = np.arange(len(cloud_masked))\n",
    "            idxs2 = np.random.choice(len(cloud_masked), self.num_points-len(cloud_masked), replace=True)\n",
    "            idxs = np.concatenate([idxs1, idxs2], axis=0)\n",
    "        cloud_sampled = cloud_masked[idxs]\n",
    "        color_sampled = color_masked[idxs]\n",
    "#         seg_sampled = seg_masked[idxs]\n",
    "#         objectness_label = seg_sampled.copy()\n",
    "#         objectness_label[objectness_label>1] = 1\n",
    "        \n",
    "#         object_poses_list = []\n",
    "#         grasp_points_list = []\n",
    "#         grasp_offsets_list = []\n",
    "#         grasp_scores_list = []\n",
    "#         grasp_tolerance_list = []\n",
    "#         for i, obj_idx in enumerate(obj_idxs):\n",
    "#             if obj_idx not in self.valid_obj_idxs:\n",
    "#                 continue\n",
    "#             if (seg_sampled == obj_idx).sum() < 50:\n",
    "#                 continue\n",
    "#             object_poses_list.append(poses[:, :, i])\n",
    "#             points, offsets, scores, tolerance = self.grasp_labels[obj_idx]\n",
    "#             collision = self.collision_labels[scene][i] #(Np, V, A, D)\n",
    "\n",
    "#             # remove invisible grasp points\n",
    "#             if self.remove_invisible:\n",
    "#                 visible_mask = remove_invisible_grasp_points(cloud_sampled[seg_sampled==obj_idx], points, poses[:,:,i], th=0.01)\n",
    "#                 points = points[visible_mask]\n",
    "#                 offsets = offsets[visible_mask]\n",
    "#                 scores = scores[visible_mask]\n",
    "#                 tolerance = tolerance[visible_mask]\n",
    "#                 collision = collision[visible_mask]\n",
    "\n",
    "#             idxs = np.random.choice(len(points), min(max(int(len(points)/4),300),len(points)), replace=False)\n",
    "#             grasp_points_list.append(points[idxs])\n",
    "#             grasp_offsets_list.append(offsets[idxs])\n",
    "#             collision = collision[idxs].copy()\n",
    "#             scores = scores[idxs].copy()\n",
    "#             scores[collision] = 0\n",
    "#             grasp_scores_list.append(scores)\n",
    "#             tolerance = tolerance[idxs].copy()\n",
    "#             tolerance[collision] = 0\n",
    "#             grasp_tolerance_list.append(tolerance)\n",
    "        \n",
    "#         if self.augment:\n",
    "#             cloud_sampled, object_poses_list = self.augment_data(cloud_sampled, object_poses_list)\n",
    "        T88 = self.loadGrasp(index).nms()\n",
    "#         for i in T88.__len__():\n",
    "#             T88[i].\n",
    "            \n",
    "        ret_dict = {}\n",
    "        ret_dict['point_clouds'] = cloud_sampled.astype(np.float32)\n",
    "        ret_dict['cloud_colors'] = color_sampled.astype(np.float32)\n",
    "#         ret_dict['objectness_label'] = objectness_label.astype(np.int64)\n",
    "#         ret_dict['object_poses_list'] = object_poses_list\n",
    "#         ret_dict['grasp_points_list'] = grasp_points_list\n",
    "#         ret_dict['grasp_offsets_list'] = grasp_offsets_list\n",
    "#         ret_dict['grasp_labels_list'] = grasp_scores_list\n",
    "#         ret_dict['grasp_tolerance_list'] = grasp_tolerance_list\n",
    "        ret_dict['grasp_list'] = T88\n",
    "\n",
    "        return ret_dict\n",
    "\n",
    "def load_grasp_labels(root):\n",
    "    obj_names = list(range(88))\n",
    "    valid_obj_idxs = []\n",
    "    grasp_labels = {}\n",
    "    for i, obj_name in enumerate(tqdm(obj_names, desc='Loading grasping labels...')):\n",
    "#         if i == 18: continue\n",
    "        valid_obj_idxs.append(i + 1) #here align with label png\n",
    "        label = np.load(os.path.join(root, 'grasp_label', '{}_labels.npz'.format(str(i).zfill(3))))\n",
    "        tolerance = np.load(os.path.join(BASE_DIR, 'tolerance', '{}_tolerance.npy'.format(str(i).zfill(3))))\n",
    "        grasp_labels[i + 1] = (label['points'].astype(np.float32), label['offsets'].astype(np.float32),\n",
    "                                label['scores'].astype(np.float32))#, tolerance)\n",
    "\n",
    "    return valid_obj_idxs, grasp_labels\n",
    "# def loadGraspLabels(self, objIds=None):\n",
    "#         '''\n",
    "#         **Input:**\n",
    "\n",
    "#         - objIds: int or list of int of the object ids.\n",
    "\n",
    "#         **Output:**\n",
    "\n",
    "#         - a dict of grasplabels of each object. \n",
    "#         '''\n",
    "#         # load object-level grasp labels of the given obj ids\n",
    "#         objIds = self.objIds if objIds is None else objIds\n",
    "#         assert _isArrayLike(objIds) or isinstance(objIds, int), 'objIds must be an integer or a list/numpy array of integers'\n",
    "#         objIds = objIds if _isArrayLike(objIds) else [objIds]\n",
    "#         graspLabels = {}\n",
    "#         for i in tqdm(objIds, desc='Loading grasping labels...'):\n",
    "#             file = np.load(os.path.join(self.root, 'grasp_label', '{}_labels.npz'.format(str(i).zfill(3))))\n",
    "#             graspLabels[i] = (file['points'].astype(np.float32), file['offsets'].astype(np.float32), file['scores'].astype(np.float32))\n",
    "#         return graspLabels\n",
    "def collate_fn(batch):\n",
    "    if type(batch[0]).__module__ == 'numpy':\n",
    "        return torch.stack([torch.from_numpy(b) for b in batch], 0)\n",
    "    elif isinstance(batch[0], container_abcs.Mapping):\n",
    "        return {key:collate_fn([d[key] for d in batch]) for key in batch[0]}\n",
    "    elif isinstance(batch[0], container_abcs.Sequence):\n",
    "        return [[torch.from_numpy(sample) for sample in b] for b in batch]\n",
    "    \n",
    "    raise TypeError(\"batch must contain tensors, dicts or lists; found {}\".format(type(batch[0])))\n",
    "    \n",
    "def distance_by_translation_point(p1, p2):\n",
    "    \"\"\"\n",
    "      Gets two nx3 points and computes the distance between point p1 and p2.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum(np.square(p1 - p2), axis=-1))\n",
    "# if __name__ == \"__main__\":\n",
    "#     root = '/home/po/TM5/graspnetAPI'\n",
    "#     valid_obj_idxs, grasp_labels = load_grasp_labels(root)\n",
    "#     train_dataset = GraspNetDataset(root, valid_obj_idxs, grasp_labels, split='train', remove_outlier=True, remove_invisible=True, num_points=20000)\n",
    "#     print(len(train_dataset))\n",
    "\n",
    "#     end_points = train_dataset[233]\n",
    "#     cloud = end_points['point_clouds']\n",
    "#     seg = end_points['objectness_label']\n",
    "#     print(cloud.shape)\n",
    "#     print(cloud.dtype)\n",
    "#     print(cloud[:,0].min(), cloud[:,0].max())\n",
    "#     print(cloud[:,1].min(), cloud[:,1].max())\n",
    "#     print(cloud[:,2].min(), cloud[:,2].max())\n",
    "#     print(seg.shape)\n",
    "#     print((seg>0).sum())\n",
    "#     print(seg.dtype)\n",
    "#     print(np.unique(seg))\n",
    "root = '/home/po/TM5/graspnetAPI'\n",
    "valid_obj_idxs, grasp_labels = load_grasp_labels(root)\n",
    "train_dataset = GraspNetDataset(root, valid_obj_idxs, grasp_labels, split='train', remove_outlier=True, remove_invisible=True, num_points=20000)\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98bbb6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyquaternion import Quaternion\n",
    "save_path = 'pre_grasp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c3d4ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(index):\n",
    "    index = index\n",
    "    grasp_list = train_dataset.__getitem__(index)['grasp_list']\n",
    "    point_clouds = train_dataset.__getitem__(index)['point_clouds']\n",
    "    template_grasp = np.zeros([20000,7])\n",
    "#     template_grasp = np.zeros([10000,7],dtype=float)\n",
    "    # import datetime\n",
    "    # starttime = datetime.datetime.now()\n",
    "    # print(\"The time used to execute this is given below\")\n",
    "    count = 0\n",
    "    label_count = 0\n",
    "#     print(grasp_list.__len__(),'\\n',len(point_clouds),'\\n',count,'\\n',label_count)\n",
    "    print(grasp_list.__len__())\n",
    "    for i in range(grasp_list.__len__()):\n",
    "        grasp_NOi = grasp_list[i]\n",
    "        xyz_NOi = grasp_NOi.translation\n",
    "        q_NOi = grasp_NOi.rotation_matrix\n",
    "#         print('grasp_i:',i)\n",
    "        for j in range(len(point_clouds)):\n",
    "            result = distance_by_translation_point(xyz_NOi,point_clouds[j])\n",
    "#             print('j:',j)\n",
    "            if result < 0.005 :\n",
    "                try:\n",
    "#                     train_dataset.template_grasp[j,3:7] = Quaternion(matrix=grasp_NOi.rotation_matrix).elements\n",
    "#                     train_dataset.template_grasp[j,0:3] = grasp_NOi.translation\n",
    "                    template_grasp[j,3:7] = Quaternion(matrix=grasp_NOi.rotation_matrix).elements\n",
    "                    template_grasp[j,0:3] = grasp_NOi.translation\n",
    "                    label_count = label_count + 1\n",
    "#                     print('count:',count,'\\n','label_count:',label_count,'\\n','i,j:',i,j,'\\n\\ng')\n",
    "#                     print('grasp_label_no:',j,'grasp_7element:',train_dataset.template_grasp[j,:])\n",
    "                    break\n",
    "                except:\n",
    "#                     print('fucki',i,'fuckj',j)\n",
    "                    break\n",
    "            count = count + 1\n",
    "#     endtime = datetime.datetime.now()\n",
    "#     print (endtime - starttime).seconds\n",
    "    \n",
    "    # save tolerance\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "    # saved_pre_grasp = [None for _ in range(len(points))]\n",
    "    # for i in range(len(points)):\n",
    "    #     saved_pre_grasp[i] = tolerance[i]\n",
    "    # saved_pre_grasp = np.array(saved_tolerance)\n",
    "    np.save('{}/{}_pre_grasp.npy'.format(save_path, index), template_grasp)\n",
    "#     print(grasp_list.__len__(),'\\n',len(point_clouds),'\\n',count,'\\n',label_count)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06281c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_pre_grasp = [None for _ in range(10)]\n",
    "saved_pre_grasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5fb1af2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e69ee9195607>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#     saved_pre_grasp[i] = tolerance[i]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# saved_pre_grasp = np.array(saved_tolerance)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}_pre_grasp.npy'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_pre_grasp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "save_path = 'pre_grasp'\n",
    "# save tolerance\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "# saved_pre_grasp = [None for _ in range(len(points))]\n",
    "# for i in range(len(points)):\n",
    "#     saved_pre_grasp[i] = tolerance[i]\n",
    "# saved_pre_grasp = np.array(saved_tolerance)\n",
    "np.save('{}/{}_pre_grasp.npy'.format(save_path, index), saved_pre_grasp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2722ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.load('{}/{}_pre_grasp.npy'.format(save_path, 0))\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b20cf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "654\n",
      "654\n",
      "653\n",
      "652\n",
      "655\n",
      "653\n",
      "653\n",
      "655\n",
      "652653\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def job(x):\n",
    "    return x*x\n",
    "def multicore():\n",
    "    pool = mp.Pool(processes=10)\n",
    "    res = pool.map(worker, range(1000))\n",
    "    print(res)\n",
    "multicore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2337440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_list = train_dataset.__getitem__(1)['grasp_list']\n",
    "point_clouds = train_dataset.__getitem__(1)['point_clouds']\n",
    "cloud_colors = train_dataset.__getitem__(1)['cloud_colors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd74337",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_grasp = np.zeros([10000,7],dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8bc78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.template_grasp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b71a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_list[0].translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56021beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_names = list(range(88))\n",
    "valid_obj_idxs = []\n",
    "grasp_labels = {}\n",
    "for i, obj_name in enumerate(tqdm(obj_names, desc='Loading grasping labels...')):\n",
    "#         if i == 18: continue\n",
    "    valid_obj_idxs.append(i + 1) #here align with label png\n",
    "    label = np.load(os.path.join(root, 'grasp_label', '{}_labels.npz'.format(str(i).zfill(3))))\n",
    "    tolerance = np.load(os.path.join(BASE_DIR, 'tolerance', '{}_tolerance.npy'.format(str(i).zfill(3))))\n",
    "    grasp_labels[i + 1] = (label['points'].astype(np.float32), label['offsets'].astype(np.float32),\n",
    "                            label['scores'].astype(np.float32))#, tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe08f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309c6b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Quaternion(matrix=grasp_list[0].rotation_matrix)\n",
    "A.elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "template_grasp[index,0:3] = grasp_list[0].translation\n",
    "template_grasp[index,3:7] = Quaternion(matrix=grasp_list[0].rotation_matrix).elements\n",
    "template_grasp[index,3:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177e8a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_clouds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adcb867",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_grasp = np.zeros([20000,7])\n",
    "worker(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e986bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_dataset = np.zeros([25600,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f84c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.template_grasp[5616,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f43ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset.frameid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90678787",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATALOADER = DataLoader(train_dataset, batch_size=2, shuffle=True,\n",
    "    num_workers=4, worker_init_fn=my_worker_init_fn, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db072dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_by_translation_point(grasp_list[652].translation,point_clouds[653])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eba4d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_rotation = TT87[651].rotation_matrix\n",
    "grasp_translation = TT87[651].translation\n",
    "print(grasp_rotation,'\\n=======================================\\n',grasp_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827e3ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "starttime = datetime.datetime.now()\n",
    "print(\"The time used to execute this is given below\")\n",
    "count = 0\n",
    "for i in range(grasp_list.__len__()):#points = 20000\n",
    "    grasp_NOi = grasp_list[i]\n",
    "    xyz_NOi = grasp_NOi.translation\n",
    "    q_NOi = grasp_NOi.rotation_matrix\n",
    "    for j in range(len(point_clouds)):\n",
    "        result = distance_by_translation_point(xyz_NOi,point_clouds[j])\n",
    "        print(result)\n",
    "        count = count + 1\n",
    "endtime = datetime.datetime.now()\n",
    "print (endtime - starttime).seconds\n",
    "print(grasp_list.__len__(),len(point_clouds),count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dfcad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation = np.eye(3)\n",
    "q8d1 = Quaternion(matrix=g_r) # Using 3x3 rotation matrix\n",
    "q8d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([1,1,1])\n",
    "B = np.array([1,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb2ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_by_translation_point(TT88[651],grasp_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc485fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_by_translation_point(p1, p2):\n",
    "    \"\"\"\n",
    "      Gets two nx3 points and computes the distance between point p1 and p2.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum(np.square(p1 - p2), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc753c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "starttime = datetime.datetime.now()\n",
    "print(\"The time used to execute this is given below\")\n",
    "for i in range(25600):\n",
    "    g_l = len(train_dataset.__getitem__(i)['grasp_list'])\n",
    "    p_c = len(train_dataset.__getitem__(i)['point_clouds'])\n",
    "    print('NUM:',i,'grasp_list:',g_l,'point_clouds:',p_c)\n",
    "    print(datetime.datetime.now())\n",
    "endtime = datetime.datetime.now()\n",
    "print (endtime - starttime).seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fe53a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "print(\"The time used to execute this is given below\")\n",
    "\n",
    "for i in range(500):\n",
    "    train_dataset.__getitem__(i)\n",
    "    print(time.time())\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ca2a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Training routine for GraspNet baseline model. \"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84df165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init datasets and dataloaders \n",
    "def my_worker_init_fn(worker_id):\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ed896",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATALOADER = DataLoader(train_dataset, batch_size=2, shuffle=True,\n",
    "    num_workers=4, worker_init_fn=my_worker_init_fn, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e01dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuck = next(iter(TRAIN_DATALOADER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3670cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch_data_label in enumerate(TRAIN_DATALOADER):\n",
    "    print( batch_idx,batch_data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea49af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for batch_idx, batch_data_label in enumerate(TRAIN_DATALOADER):\n",
    "        for key in batch_data_label:\n",
    "            if 'list' in key:\n",
    "                for i in range(len(batch_data_label[key])):\n",
    "                    for j in range(len(batch_data_label[key][i])):\n",
    "                        batch_data_label[key][i][j] = batch_data_label[key][i][j].to(device)\n",
    "            else:\n",
    "                batch_data_label[key] = batch_data_label[key].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f95cce8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
